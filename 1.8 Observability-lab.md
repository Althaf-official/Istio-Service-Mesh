### Which of the following addons can be used to visualize the Istio service mesh traffic data?

Grafana

### Which of the following types of telemetry does Istio generate to provide the overall service mesh observability?



Access Logs


Distributed Traces


Metrics


### Which one of the following metrics is not provided by Istio ?

Istio doesn't provide kubernetes Cluster-level metrics.

### Which of the following tracing back-ends is supported by Istio?

Istio supports a number of tracing backends, including Zipkin, Jaeger, Lightstep, and Datadog.


### We have the prometheus.yml template under /root directory. Setup prometheus using this template and try to access the Prometheus dashboard.


Filter out different metrics in Prometheus and checkout the graphs/metrics, if you are able to see some data there.

```ruby
root@controlplane ~ ➜  ls
gateway.yml  istio-1.20.2  kiali-svc.yml  prometheus.yml  virtual-service.yml

root@controlplane ~ ➜  cat prometheus.yml 
---
# Source: prometheus/templates/server/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
  namespace: istio-system
  annotations:
    {}
---
# Source: prometheus/templates/server/cm.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
  namespace: istio-system
data:
  alerting_rules.yml: |
    {}
  alerts: |
    {}
  prometheus.yml: |
    global:
      evaluation_interval: 1m
      scrape_interval: 15s
      scrape_timeout: 10s
    rule_files:
    - /etc/config/recording_rules.yml
    - /etc/config/alerting_rules.yml
    - /etc/config/rules
    - /etc/config/alerts
    scrape_configs:
    - job_name: prometheus
      static_configs:
      - targets:
        - localhost:9090
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-apiservers
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: default;kubernetes;https
        source_labels:
        - __meta_kubernetes_namespace
        - __meta_kubernetes_service_name
        - __meta_kubernetes_endpoint_port_name
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
      job_name: kubernetes-nodes-cadvisor
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
      - replacement: kubernetes.default.svc:443
        target_label: __address__
      - regex: (.+)
        replacement: /api/v1/nodes/$1/proxy/metrics/cadvisor
        source_labels:
        - __meta_kubernetes_node_name
        target_label: __metrics_path__
      scheme: https
      tls_config:
        ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        insecure_skip_verify: true
    - job_name: kubernetes-service-endpoints
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
    - job_name: kubernetes-service-endpoints-slow
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_service_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_node_name
        target_label: kubernetes_node
      scrape_interval: 5m
      scrape_timeout: 30s
    - honor_labels: true
      job_name: prometheus-pushgateway
      kubernetes_sd_configs:
      - role: service
      relabel_configs:
      - action: keep
        regex: pushgateway
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
    - job_name: kubernetes-services
      kubernetes_sd_configs:
      - role: service
      metrics_path: /probe
      params:
        module:
        - http_2xx
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_service_annotation_prometheus_io_probe
      - source_labels:
        - __address__
        target_label: __param_target
      - replacement: blackbox
        target_label: __address__
      - source_labels:
        - __param_target
        target_label: instance
      - action: labelmap
        regex: __meta_kubernetes_service_label_(.+)
      - source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - source_labels:
        - __meta_kubernetes_service_name
        target_label: kubernetes_name
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
    - job_name: kubernetes-pods-slow
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - action: keep
        regex: true
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scrape_slow
      - action: replace
        regex: (https?)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_scheme
        target_label: __scheme__
      - action: replace
        regex: (.+)
        source_labels:
        - __meta_kubernetes_pod_annotation_prometheus_io_path
        target_label: __metrics_path__
      - action: replace
        regex: ([^:]+)(?::\d+)?;(\d+)
        replacement: $1:$2
        source_labels:
        - __address__
        - __meta_kubernetes_pod_annotation_prometheus_io_port
        target_label: __address__
      - action: labelmap
        regex: __meta_kubernetes_pod_label_(.+)
      - action: replace
        source_labels:
        - __meta_kubernetes_namespace
        target_label: kubernetes_namespace
      - action: replace
        source_labels:
        - __meta_kubernetes_pod_name
        target_label: kubernetes_pod_name
      - action: drop
        regex: Pending|Succeeded|Failed
        source_labels:
        - __meta_kubernetes_pod_phase
      scrape_interval: 5m
      scrape_timeout: 30s
  recording_rules.yml: |
    {}
  rules: |
    {}
---
# Source: prometheus/templates/server/clusterrole.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
rules:
  - apiGroups:
      - ""
    resources:
      - nodes
      - nodes/proxy
      - nodes/metrics
      - services
      - endpoints
      - pods
      - ingresses
      - configmaps
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - "extensions"
      - "networking.k8s.io"
    resources:
      - ingresses/status
      - ingresses
    verbs:
      - get
      - list
      - watch
  - nonResourceURLs:
      - "/metrics"
    verbs:
      - get
---
# Source: prometheus/templates/server/clusterrolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
subjects:
  - kind: ServiceAccount
    name: prometheus
    namespace: istio-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
---
# Source: prometheus/templates/server/service.yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
  namespace: istio-system
spec:
  ports:
    - name: http
      port: 9090
      protocol: TCP
      targetPort: 9090
      nodePort: 30009
  selector:
    component: "server"
    app: prometheus
    release: prometheus
  sessionAffinity: None
  type: "NodePort"
---
# Source: prometheus/templates/server/deploy.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    component: "server"
    app: prometheus
    release: prometheus
    chart: prometheus-14.3.0
    heritage: Helm
  name: prometheus
  namespace: istio-system
spec:
  selector:
    matchLabels:
      component: "server"
      app: prometheus
      release: prometheus
  replicas: 1
  template:
    metadata:
      annotations:
        
        sidecar.istio.io/inject: "false"
      labels:
        component: "server"
        app: prometheus
        release: prometheus
        chart: prometheus-14.3.0
        heritage: Helm
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus-server-configmap-reload
          image: "jimmidyson/configmap-reload:v0.5.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --volume-dir=/etc/config
            - --webhook-url=http://127.0.0.1:9090/-/reload
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
              readOnly: true

        - name: prometheus-server
          image: "prom/prometheus:v2.26.0"
          imagePullPolicy: "IfNotPresent"
          args:
            - --storage.tsdb.retention.time=15d
            - --config.file=/etc/config/prometheus.yml
            - --storage.tsdb.path=/data
            - --web.console.libraries=/etc/prometheus/console_libraries
            - --web.console.templates=/etc/prometheus/consoles
            - --web.enable-lifecycle
          ports:
            - containerPort: 9090
          readinessProbe:
            httpGet:
              path: /-/ready
              port: 9090
            initialDelaySeconds: 0
            periodSeconds: 5
            timeoutSeconds: 4
            failureThreshold: 3
            successThreshold: 1
          livenessProbe:
            httpGet:
              path: /-/healthy
              port: 9090
            initialDelaySeconds: 30
            periodSeconds: 15
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1
          resources:
            {}
          volumeMounts:
            - name: config-volume
              mountPath: /etc/config
            - name: storage-volume
              mountPath: /data
              subPath: ""
      hostNetwork: false
      dnsPolicy: ClusterFirst
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
      terminationGracePeriodSeconds: 300
      volumes:
        - name: config-volume
          configMap:
            name: prometheus
        - name: storage-volume
          emptyDir:
            {}

root@controlplane ~ ➜  

root@controlplane ~ ➜  

root@controlplane ~ ➜  kubectl get all
NAME                           READY   STATUS    RESTARTS   AGE
pod/httpbin-86869bccff-nv7dc   2/2     Running   0          9m40s

NAME                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/httpbin      ClusterIP   10.111.152.94   <none>        8000/TCP   9m40s
service/kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP    3h29m

NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/httpbin   1/1     1            1           9m41s

NAME                                 DESIRED   CURRENT   READY   AGE
replicaset.apps/httpbin-86869bccff   1         1         1       9m41s

root@controlplane ~ ➜  

root@controlplane ~ ➜  

root@controlplane ~ ➜  kubectl get all -n istio-system
NAME                                        READY   STATUS    RESTARTS   AGE
pod/istio-egressgateway-975ff4944-sx6wc     1/1     Running   0          10m
pod/istio-ingressgateway-6cb9989d98-52fwq   1/1     Running   0          10m
pod/istiod-855f4b5f7-rn8d8                  1/1     Running   0          11m
pod/kiali-cc67f8648-t7ms2                   1/1     Running   0          10m

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/istio-egressgateway   ClusterIP   10.98.60.83      <none>        80/TCP,443/TCP                          10m
service/istiod                ClusterIP   10.96.15.159     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   11m
service/kiali                 ClusterIP   10.108.248.243   <none>        20001/TCP,9090/TCP                      10m
service/kiali-svc             NodePort    10.110.216.92    <none>        20001:30007/TCP,9090:30008/TCP          10m
service/prometheus            NodePort    10.110.207.205   <none>        9090:30009/TCP                          10m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/istio-egressgateway    1/1     1            1           10m
deployment.apps/istio-ingressgateway   1/1     1            1           10m
deployment.apps/istiod                 1/1     1            1           11m
deployment.apps/kiali                  1/1     1            1           10m

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/istio-egressgateway-975ff4944     1         1         1       10m
replicaset.apps/istio-ingressgateway-6cb9989d98   1         1         1       10m
replicaset.apps/istiod-855f4b5f7                  1         1         1       11m
replicaset.apps/kiali-cc67f8648                   1         1         1       10m

root@controlplane ~ ➜  kubectl apply -f prometheus.yml 
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus configured
deployment.apps/prometheus created

root@controlplane ~ ➜  kubectl get all -n istio-system
NAME                                        READY   STATUS              RESTARTS   AGE
pod/istio-egressgateway-975ff4944-sx6wc     1/1     Running             0          12m
pod/istio-ingressgateway-6cb9989d98-52fwq   1/1     Running             0          12m
pod/istiod-855f4b5f7-rn8d8                  1/1     Running             0          12m
pod/kiali-cc67f8648-t7ms2                   1/1     Running             0          12m
pod/prometheus-5986b7bc46-j27hx             0/2     ContainerCreating   0          7s

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/istio-egressgateway   ClusterIP   10.98.60.83      <none>        80/TCP,443/TCP                          12m
service/istiod                ClusterIP   10.96.15.159     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   12m
service/kiali                 ClusterIP   10.108.248.243   <none>        20001/TCP,9090/TCP                      12m
service/kiali-svc             NodePort    10.110.216.92    <none>        20001:30007/TCP,9090:30008/TCP          12m
service/prometheus            NodePort    10.110.207.205   <none>        9090:30009/TCP                          12m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/istio-egressgateway    1/1     1            1           12m
deployment.apps/istio-ingressgateway   1/1     1            1           12m
deployment.apps/istiod                 1/1     1            1           12m
deployment.apps/kiali                  1/1     1            1           12m
deployment.apps/prometheus             0/1     1            0           7s

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/istio-egressgateway-975ff4944     1         1         1       12m
replicaset.apps/istio-ingressgateway-6cb9989d98   1         1         1       12m
replicaset.apps/istiod-855f4b5f7                  1         1         1       12m
replicaset.apps/kiali-cc67f8648                   1         1         1       12m
replicaset.apps/prometheus-5986b7bc46             1         1         0       7s

root@controlplane ~ ➜  

root@controlplane ~ ➜  kubectl get all -n istio-system
NAME                                        READY   STATUS    RESTARTS   AGE
pod/istio-egressgateway-975ff4944-sx6wc     1/1     Running   0          12m
pod/istio-ingressgateway-6cb9989d98-52fwq   1/1     Running   0          12m
pod/istiod-855f4b5f7-rn8d8                  1/1     Running   0          13m
pod/kiali-cc67f8648-t7ms2                   1/1     Running   0          12m
pod/prometheus-5986b7bc46-j27hx             2/2     Running   0          20s

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/istio-egressgateway   ClusterIP   10.98.60.83      <none>        80/TCP,443/TCP                          12m
service/istiod                ClusterIP   10.96.15.159     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   13m
service/kiali                 ClusterIP   10.108.248.243   <none>        20001/TCP,9090/TCP                      12m
service/kiali-svc             NodePort    10.110.216.92    <none>        20001:30007/TCP,9090:30008/TCP          12m
service/prometheus            NodePort    10.110.207.205   <none>        9090:30009/TCP                          12m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/istio-egressgateway    1/1     1            1           12m
deployment.apps/istio-ingressgateway   1/1     1            1           12m
deployment.apps/istiod                 1/1     1            1           13m
deployment.apps/kiali                  1/1     1            1           12m
deployment.apps/prometheus             1/1     1            1           20s

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/istio-egressgateway-975ff4944     1         1         1       12m
replicaset.apps/istio-ingressgateway-6cb9989d98   1         1         1       12m
replicaset.apps/istiod-855f4b5f7                  1         1         1       13m
replicaset.apps/kiali-cc67f8648                   1         1         1       12m
replicaset.apps/prometheus-5986b7bc46             1         1         1       20s

root@controlplane ~ ➜  
```

![image](https://github.com/Althaf-official/Istio-Service-Mesh/assets/105126131/f0d08058-957c-4308-b661-c0223d9b7560)


### Now let's setup Grafana to view some graphs and data. We have a template grafana.yaml under /root directory. Apply that template to setup Grafana.


Access the Grafana dashboard using Grafana button on the top bar. Under search you should be able to see some default istio related dashboards, play around with graphs to see different types of data.

grafana.yaml 

```ruby
 Source: grafana/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    helm.sh/chart: grafana-6.11.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "7.5.5"
    app.kubernetes.io/managed-by: Helm
  name: grafana
  namespace: istio-system
---
# Source: grafana/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana
  namespace: istio-system
  labels:
    helm.sh/chart: grafana-6.11.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "7.5.5"
    app.kubernetes.io/managed-by: Helm
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    [grafana_net]
    url = https://grafana.net
    [log]
    mode = console
    [paths]
    data = /var/lib/grafana/
    logs = /var/log/grafana
    plugins = /var/lib/grafana/plugins
    provisioning = /etc/grafana/provisioning

  datasources.yaml: |
    apiVersion: 1
    datasources:
    - access: proxy
      editable: true
      isDefault: true
      jsonData:
        timeInterval: 5s
      name: Prometheus
      orgId: 1
      type: prometheus
      url: http://prometheus:9090
  dashboardproviders.yaml: |
    apiVersion: 1
    providers:
    - disableDeletion: false
      folder: istio
      name: istio
      options:
        path: /var/lib/grafana/dashboards/istio
      orgId: 1
      type: file
    - disableDeletion: false
      folder: istio
      name: istio-services
      options:
        path: /var/lib/grafana/dashboards/istio-services
      orgId: 1
      type: file
---
# Source: grafana/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: istio-system
  labels:
    helm.sh/chart: grafana-6.11.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "7.5.5"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - name: service
      port: 3000
      protocol: TCP
      targetPort: 3000
      nodePort: 30010

  selector:
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
---
# Source: grafana/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: istio-system
  labels:
    helm.sh/chart: grafana-6.11.0
    app.kubernetes.io/name: grafana
    app.kubernetes.io/instance: grafana
    app.kubernetes.io/version: "7.5.5"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: grafana
      app.kubernetes.io/instance: grafana
  strategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/name: grafana
        app.kubernetes.io/instance: grafana
        app: grafana
      annotations:
        checksum/config: af4530cc6e67e63b6285f3ceccaf0aaa2a20d322e35c9f0ae48721add3b58eb0
        checksum/dashboards-json-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        checksum/sc-dashboard-provider-config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        sidecar.istio.io/inject: "false"
    spec:

      serviceAccountName: grafana
      securityContext:
        fsGroup: 472
        runAsGroup: 472
        runAsUser: 472
      containers:
        - name: grafana
          image: "grafana/grafana:7.5.5"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: config
              mountPath: "/etc/grafana/grafana.ini"
              subPath: grafana.ini
            - name: storage
              mountPath: "/var/lib/grafana"
            - name: dashboards-istio
              mountPath: "/var/lib/grafana/dashboards/istio"
            - name: dashboards-istio-services
              mountPath: "/var/lib/grafana/dashboards/istio-services"
            - name: config
              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
              subPath: datasources.yaml
            - name: config
              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
              subPath: dashboardproviders.yaml
          ports:
            - name: service
              containerPort: 3000
              protocol: TCP
            - name: grafana
              containerPort: 3000
              protocol: TCP
          env:

            - name: GF_PATHS_DATA
              value: /var/lib/grafana/
            - name: GF_PATHS_LOGS
              value: /var/log/grafana
            - name: GF_PATHS_PLUGINS
              value: /var/lib/grafana/plugins
            - name: GF_PATHS_PROVISIONING
              value: /etc/grafana/provisioning
            - name: "GF_AUTH_ANONYMOUS_ENABLED"
              value: "true"
            - name: "GF_AUTH_ANONYMOUS_ORG_ROLE"
              value: "Admin"
            - name: "GF_AUTH_BASIC_ENABLED"
              value: "false"
            - name: "GF_SECURITY_ADMIN_PASSWORD"
              value: "-"
            - name: "GF_SECURITY_ADMIN_USER"
              value: "-"
          livenessProbe:
            failureThreshold: 10
            httpGet:
              path: /api/health
              port: 3000
            initialDelaySeconds: 60
            timeoutSeconds: 30
          readinessProbe:
            httpGet:
              path: /api/health
              port: 3000
          resources:
            {}
      volumes:
        - name: config
          configMap:
            name: grafana

        - name: dashboards-istio
          configMap:
            name: istio-grafana-dashboards
        - name: dashboards-istio-services
          configMap:
            name: istio-services-grafana-dashboards
        - name: storage
          emptyDir: {}

---

```


----------------------------------

```ruby
root@controlplane ~ ➜  vi grafana.yaml 

root@controlplane ~ ➜  kubectl apply -f /root/grafana.yaml
serviceaccount/grafana unchanged
configmap/grafana unchanged
service/grafana unchanged
deployment.apps/grafana configured
configmap/istio-grafana-dashboards configured
configmap/istio-services-grafana-dashboards configured

root@controlplane ~ ➜  
```


![image](https://github.com/Althaf-official/Istio-Service-Mesh/assets/105126131/25e05eaf-774b-4abf-9f04-201e522d7765)


### We have httpbin application setup under default namespace. Now let's setup Jaeger to view some traces of this service. We have a template jaeger.yml under /root directory. Apply that template to setup Jaeger.


Hit the httpbin using a curl command to generate some traces (i.e curl <cluster-ip-of-svc>:<port>), you can identify the service IP by listing the services under default namespace (kubectl get svc). Access the Jaeger dashboard using Jaeger button on the top bar.

```ruby
root@controlplane ~ ➜  ls
gateway.yml   istio-1.20.2  kiali-svc.yml   virtual-service.yml
grafana.yaml  jaeger.yml    prometheus.yml

root@controlplane ~ ➜  cat jaeger.yml 
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: istio-system
  labels:
    app: jaeger
spec:
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
      annotations:
        sidecar.istio.io/inject: "false"
        prometheus.io/scrape: "true"
        prometheus.io/port: "14269"
    spec:
      containers:
        - name: jaeger
          image: "docker.io/jaegertracing/all-in-one:1.23"
          env:
            - name: BADGER_EPHEMERAL
              value: "false"
            - name: SPAN_STORAGE_TYPE
              value: "badger"
            - name: BADGER_DIRECTORY_VALUE
              value: "/badger/data"
            - name: BADGER_DIRECTORY_KEY
              value: "/badger/key"
            - name: COLLECTOR_ZIPKIN_HOST_PORT
              value: ":9411"
            - name: MEMORY_MAX_TRACES
              value: "50000"
            - name: QUERY_BASE_PATH
              value: /jaeger
          livenessProbe:
            httpGet:
              path: /
              port: 14269
          readinessProbe:
            httpGet:
              path: /
              port: 14269
          volumeMounts:
            - name: data
              mountPath: /badger
          resources:
            requests:
              cpu: 10m
      volumes:
        - name: data
          emptyDir: {}
---
apiVersion: v1
kind: Service
metadata:
  name: tracing
  namespace: istio-system
  labels:
    app: jaeger
spec:
  type: NodePort
  ports:
    - name: http-query
      port: 80
      protocol: TCP
      targetPort: 16686
      nodePort: 30006
    # Note: Change port name if you add '--query.grpc.tls.enabled=true'
    - name: grpc-query
      port: 16685
      protocol: TCP
      targetPort: 16685
  selector:
    app: jaeger
---
# Jaeger implements the Zipkin API. To support swapping out the tracing backend, we use a Service named Zipkin.
apiVersion: v1
kind: Service
metadata:
  labels:
    name: zipkin
  name: zipkin
  namespace: istio-system
spec:
  ports:
    - port: 9411
      targetPort: 9411
      name: http-query
  selector:
    app: jaeger
---
apiVersion: v1
kind: Service
metadata:
  name: jaeger-collector
  namespace: istio-system
  labels:
    app: jaeger
spec:
  type: ClusterIP
  ports:
  - name: jaeger-collector-http
    port: 14268
    targetPort: 14268
    protocol: TCP
  - name: jaeger-collector-grpc
    port: 14250
    targetPort: 14250
    protocol: TCP
  - port: 9411
    targetPort: 9411
    name: http-zipkin
  selector:
    app: jaeger

root@controlplane ~ ➜  kubectl get all -n istio-system 
NAME                                        READY   STATUS    RESTARTS   AGE
pod/grafana-778686fd69-56cws                1/1     Running   0          19m
pod/istio-egressgateway-975ff4944-sx6wc     1/1     Running   0          35m
pod/istio-ingressgateway-6cb9989d98-52fwq   1/1     Running   0          35m
pod/istiod-855f4b5f7-rn8d8                  1/1     Running   0          36m
pod/kiali-cc67f8648-t7ms2                   1/1     Running   0          35m
pod/prometheus-5986b7bc46-j27hx             2/2     Running   0          23m

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/grafana               NodePort    10.107.186.244   <none>        3000:30010/TCP                          19m
service/istio-egressgateway   ClusterIP   10.98.60.83      <none>        80/TCP,443/TCP                          35m
service/istiod                ClusterIP   10.96.15.159     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   36m
service/kiali                 ClusterIP   10.108.248.243   <none>        20001/TCP,9090/TCP                      35m
service/kiali-svc             NodePort    10.110.216.92    <none>        20001:30007/TCP,9090:30008/TCP          35m
service/prometheus            NodePort    10.110.207.205   <none>        9090:30009/TCP                          35m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana                1/1     1            1           19m
deployment.apps/istio-egressgateway    1/1     1            1           35m
deployment.apps/istio-ingressgateway   1/1     1            1           35m
deployment.apps/istiod                 1/1     1            1           36m
deployment.apps/kiali                  1/1     1            1           35m
deployment.apps/prometheus             1/1     1            1           23m

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/grafana-778686fd69                1         1         1       19m
replicaset.apps/istio-egressgateway-975ff4944     1         1         1       35m
replicaset.apps/istio-ingressgateway-6cb9989d98   1         1         1       35m
replicaset.apps/istiod-855f4b5f7                  1         1         1       36m
replicaset.apps/kiali-cc67f8648                   1         1         1       35m
replicaset.apps/prometheus-5986b7bc46             1         1         1       23m

root@controlplane ~ ➜  kubectl apply -f /root/jaeger.yml
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created

root@controlplane ~ ➜  kubectl get all -n istio-system 
NAME                                        READY   STATUS    RESTARTS   AGE
pod/grafana-778686fd69-56cws                1/1     Running   0          19m
pod/istio-egressgateway-975ff4944-sx6wc     1/1     Running   0          36m
pod/istio-ingressgateway-6cb9989d98-52fwq   1/1     Running   0          36m
pod/istiod-855f4b5f7-rn8d8                  1/1     Running   0          36m
pod/jaeger-68f78dcdbc-7w46w                 1/1     Running   0          9s
pod/kiali-cc67f8648-t7ms2                   1/1     Running   0          36m
pod/prometheus-5986b7bc46-j27hx             2/2     Running   0          23m

NAME                          TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                                 AGE
service/grafana               NodePort    10.107.186.244   <none>        3000:30010/TCP                          19m
service/istio-egressgateway   ClusterIP   10.98.60.83      <none>        80/TCP,443/TCP                          36m
service/istiod                ClusterIP   10.96.15.159     <none>        15010/TCP,15012/TCP,443/TCP,15014/TCP   36m
service/jaeger-collector      ClusterIP   10.111.215.193   <none>        14268/TCP,14250/TCP,9411/TCP            9s
service/kiali                 ClusterIP   10.108.248.243   <none>        20001/TCP,9090/TCP                      36m
service/kiali-svc             NodePort    10.110.216.92    <none>        20001:30007/TCP,9090:30008/TCP          36m
service/prometheus            NodePort    10.110.207.205   <none>        9090:30009/TCP                          36m
service/tracing               NodePort    10.102.43.249    <none>        80:30006/TCP,16685:31588/TCP            9s
service/zipkin                ClusterIP   10.111.119.249   <none>        9411/TCP                                9s

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/grafana                1/1     1            1           19m
deployment.apps/istio-egressgateway    1/1     1            1           36m
deployment.apps/istio-ingressgateway   1/1     1            1           36m
```

![image](https://github.com/Althaf-official/Istio-Service-Mesh/assets/105126131/d2af4969-c0ef-4b4d-9b8c-a4f0abf649a2)


